{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb951d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d57884",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"The cat sat on my face\"\n",
    "doc_b = \"The dog sat on my bed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_a = doc_a\n",
    "bow_b = doc_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c29251",
   "metadata": {},
   "source": [
    "### Merge two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set(bow_a).union(set(bow_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df977f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd97ab5",
   "metadata": {},
   "source": [
    "### Convert to dictionary and set initial values to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict_a = dict.fromkeys(word_set, 0) \n",
    "word_dict_b = dict.fromkeys(word_set, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972302a9",
   "metadata": {},
   "source": [
    "### Get count of words per each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in bow_a:\n",
    "    word_dict_a[word]+=1\n",
    "    \n",
    "for word in bow_b:\n",
    "    word_dict_b[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caec6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0dfb9",
   "metadata": {},
   "source": [
    "### Converting to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([word_dict_a, word_dict_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a93e7",
   "metadata": {},
   "source": [
    "### Compute Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94502d",
   "metadata": {},
   "source": [
    "![alt text](tf.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(word_dict, bow):\n",
    "    tf_dict = {}\n",
    "    bow_count = len(bow)\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count/float(bow_count)\n",
    "    \n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "### bow_a = The cat sat on my face\n",
    "\n",
    "tf_bow_a = (word_dict_a, bow_a)\n",
    "tf_bow_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02638f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The dog sat on my bed\n",
    "\n",
    "tf_bow_b = (word_dict_b, bow_b)\n",
    "tf_bow_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edbbd5e",
   "metadata": {},
   "source": [
    "### Compute IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58166e16",
   "metadata": {},
   "source": [
    "![alt text](idf.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(doc_list):\n",
    "    idf_dict = {}\n",
    "    \n",
    "    number_of_items = len(doc_list)  ### 2\n",
    "    \n",
    "    idf_dict = dict.fromkeys(doc_list[0].keys(), 0)\n",
    "    for doc in doc_list:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idf_dict[word] += 1\n",
    "                \n",
    "    print(idf_dict)\n",
    "    \n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log10(number_of_items / float(val))\n",
    "        \n",
    "    return idf_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = ([word_dict_a, word_dict_b])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362920af",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd177e",
   "metadata": {},
   "source": [
    "![alt text](tfidf.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tf_bow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tf_bow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "        \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5de3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_bow_a = (tf_bow_a, idfs)\n",
    "tfidf_bow_b = (tf_bow_b, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([tfidf_bow_a, tfidf_bow_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e9abb",
   "metadata": {},
   "source": [
    "### Using Scikit Learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59331244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('twitter_training.csv', header=None, usecols=[2,3])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a965512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.columns = ['Sentiment', 'Text']\n",
    "\n",
    "# remove missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# encode target label\n",
    "le = LabelEncoder()\n",
    "df['Sentiment'] = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# establish input and output\n",
    "X = list(df['Text'])\n",
    "y = list(df['Sentiment'])\n",
    "\n",
    "# split data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bag of words for with unigrams and bigrams\n",
    "tfidf = (sublinear_tf=True,norm='l2',encoding='utf-8',ngram_range=(1,1), \n",
    "                        stop_words='english')\n",
    "\n",
    "# convert training data to bag of words\n",
    "X_train_cv = tfidf.fit_transform(X_train)\n",
    "X_test_cv = tfidf.transform(X_test)\n",
    "\n",
    "# train naive bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_cv, y_train)\n",
    "\n",
    "# create predictions\n",
    "y_pred = clf.predict(X_test_cv)\n",
    "\n",
    "# find f-1 score\n",
    "score = f1_score(y_test, y_pred, average='micro')\n",
    "print('F-1 score : {}'.format(np.round(score,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d577805",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in range(1,11):\n",
    "    \n",
    "    # convert training data to bag of words\n",
    "    tfidf = (sublinear_tf=True,norm='l2',encoding='utf-8',ngram_range=(1,N), \n",
    "                        stop_words='english')\n",
    "    X_train_cv = tfidf.fit_transform(X_train)\n",
    "    X_test_cv = tfidf.transform(X_test)\n",
    "    \n",
    "    # train model and generate predictions\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_cv, y_train)\n",
    "    y_pred = clf.predict(X_test_cv)\n",
    "    \n",
    "    # compute f-1 score\n",
    "    score = np.round(f1_score(y_test, y_pred, average='micro'), 4)\n",
    "    print('F-1 score of model with n-gram range of {}: {}'.format((1,N), score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
