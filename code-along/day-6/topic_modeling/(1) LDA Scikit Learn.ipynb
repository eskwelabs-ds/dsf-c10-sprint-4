{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c28aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text preprocessing\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import  \n",
    "from nltk.stem.wordnet import \n",
    "import string\n",
    "\n",
    "# import vectorizers\n",
    "from sklearn.feature_extraction.text import \n",
    "\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# import LDA from sklearn\n",
    "from sklearn.decomposition import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e111dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 'I want to watch a movie this weekend.'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the documents into a list:\n",
    "corpus = [D1, D2, D3, D4, D5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58853500",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing\n",
    "\n",
    "Steps to preprocess text data:\n",
    "\n",
    "1. Convert the text into lowercase\n",
    "2. Split text into words\n",
    "3. Remove the stop  words\n",
    "3. Remove the Punctuation, any symbols and special characters\n",
    "4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Preprocessing on the Corpus\n",
    "\n",
    "# stop  words \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# punctuation \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# lemmatization\n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return normalized\n",
    "\n",
    "# clean data stored in a new list\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf42939",
   "metadata": {},
   "source": [
    "### 3. Convert Text into Numerical Representation\n",
    "\n",
    "Converting the clean preprocessed corpus to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cca763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text into numerical representation\n",
    "tf_idf_vectorizer = (tokenizer=lambda doc: doc, lowercase=False)\n",
    "\n",
    "tf_idf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c22485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array from TF-IDF Vectorizer \n",
    "tf_idf_arr = tf_idf_vectorizer.(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vocabulary array which will represent all the corpus \n",
    "vocab_tf_idf = tf_idf_vectorizer.()\n",
    "\n",
    "# get the vocb list\n",
    "vocab_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(vocab_tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982ccad",
   "metadata": {},
   "source": [
    "### 4. Implementation of LDA\n",
    "\n",
    "To implement LDA, pass the corpus: document-term matrix to the model. We had above obtained the unique words of vocabulary using both TF-IDF and Count Vectorizer. We can continue with either as have the same unique words in both the obtained vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3487e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object for the LDA class \n",
    "# Inside this class LDA: define the components:\n",
    "lda_model = (n_components = 3, max_iter = 20, random_state = 20)\n",
    "\n",
    "# fit transform on model on our count_vectorizer : running this will return our topics \n",
    "X_topics = lda_model.(tf_idf_arr)\n",
    "\n",
    "# .components_ gives us our topic distribution \n",
    "topic_words = lda_model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea806c5",
   "metadata": {},
   "source": [
    "### 4a. Retrieve the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7255ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define the number of Words that we want to print in every topic : n_top_words\n",
    "n_top_words = 5\n",
    "\n",
    "for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "    # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "    sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "    # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "    topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "    # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "    # obtaining topics + words\n",
    "    # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "    topic_words = topic_words[:-n_top_words:-1]\n",
    "    print (\"Topic\", str(i+1), topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af52aa",
   "metadata": {},
   "source": [
    "### 4b. Annotating the topics the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d62088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view what topics are assigned to the douments:\n",
    "\n",
    "doc_topic = lda_model.(tf_idf_arr)  \n",
    "\n",
    "# iterating over ever value till the end value\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    \n",
    "    # argmax() gives maximum index value\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    \n",
    "    # document is n+1  \n",
    "    print (\"Document\", n+1, \" -- Topic:\" ,topic_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
