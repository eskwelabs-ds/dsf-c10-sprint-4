{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c28aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text preprocessing\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import string\n",
    "\n",
    "# import vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# import LDA from sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e111dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502a19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 'I want to watch a movie this weekend.'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d1b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the documents into a list:\n",
    "corpus = [D1, D2, D3, D4, D5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c827ec61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to watch a movie this weekend.',\n",
       " 'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.',\n",
       " 'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.',\n",
       " 'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!',\n",
       " 'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58853500",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing\n",
    "\n",
    "Steps to preprocess text data:\n",
    "\n",
    "1. Convert the text into lowercase\n",
    "2. Split text into words\n",
    "3. Remove the stop  words\n",
    "3. Remove the Punctuation, any symbols and special characters\n",
    "4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738f1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Preprocessing on the Corpus\n",
    "\n",
    "# stop  words \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# punctuation \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# lemmatization\n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return normalized\n",
    "\n",
    "# clean data stored in a new list\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1669fb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'watch', 'movie', 'weekend'],\n",
       " ['went',\n",
       "  'shopping',\n",
       "  'yesterday',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'world',\n",
       "  'test',\n",
       "  'championship',\n",
       "  'beating',\n",
       "  'india',\n",
       "  'eight',\n",
       "  'wicket',\n",
       "  'southampton'],\n",
       " ['don’t',\n",
       "  'watch',\n",
       "  'cricket',\n",
       "  'netflix',\n",
       "  'amazon',\n",
       "  'prime',\n",
       "  'good',\n",
       "  'movie',\n",
       "  'watch'],\n",
       " ['movie',\n",
       "  'nice',\n",
       "  'way',\n",
       "  'chill',\n",
       "  'however',\n",
       "  'time',\n",
       "  'would',\n",
       "  'like',\n",
       "  'paint',\n",
       "  'read',\n",
       "  'good',\n",
       "  'book',\n",
       "  'it’s',\n",
       "  'long'],\n",
       " ['blueberry',\n",
       "  'milkshake',\n",
       "  'good',\n",
       "  'try',\n",
       "  'reading',\n",
       "  'dr',\n",
       "  'joe',\n",
       "  'dispenza’s',\n",
       "  'book',\n",
       "  'work',\n",
       "  'gamechanger',\n",
       "  'book',\n",
       "  'helped',\n",
       "  'learn',\n",
       "  'much',\n",
       "  'thought',\n",
       "  'impact',\n",
       "  'biology',\n",
       "  'rewire',\n",
       "  'brain']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf42939",
   "metadata": {},
   "source": [
    "### 3. Convert Text into Numerical Representation\n",
    "\n",
    "Converting the clean preprocessed corpus to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26cca763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(lowercase=False,\n",
       "                tokenizer=&lt;function &lt;lambda&gt; at 0x000001C076726F70&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False,\n",
       "                tokenizer=&lt;function &lt;lambda&gt; at 0x000001C076726F70&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(lowercase=False,\n",
       "                tokenizer=<function <lambda> at 0x000001C076726F70>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting text into numerical representation\n",
    "tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "tf_idf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c22485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array from TF-IDF Vectorizer \n",
    "tf_idf_arr = tf_idf_vectorizer.fit_transform(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f81caa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rheyannmagcalas\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['amazon',\n",
       " 'beating',\n",
       " 'biology',\n",
       " 'blueberry',\n",
       " 'book',\n",
       " 'brain',\n",
       " 'championship',\n",
       " 'chill',\n",
       " 'cricket',\n",
       " 'dispenza’s',\n",
       " 'don’t',\n",
       " 'dr',\n",
       " 'eight',\n",
       " 'gamechanger',\n",
       " 'good',\n",
       " 'helped',\n",
       " 'however',\n",
       " 'impact',\n",
       " 'india',\n",
       " 'it’s',\n",
       " 'joe',\n",
       " 'learn',\n",
       " 'like',\n",
       " 'long',\n",
       " 'milkshake',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'netflix',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'paint',\n",
       " 'prime',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'rewire',\n",
       " 'shopping',\n",
       " 'southampton',\n",
       " 'test',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'try',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'weekend',\n",
       " 'went',\n",
       " 'wicket',\n",
       " 'work',\n",
       " 'world',\n",
       " 'would',\n",
       " 'yesterday',\n",
       " 'zealand']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating vocabulary array which will represent all the corpus \n",
    "vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "\n",
    "# get the vocb list\n",
    "vocab_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45df160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(vocab_tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982ccad",
   "metadata": {},
   "source": [
    "### 4. Implementation of LDA\n",
    "\n",
    "To implement LDA, pass the corpus: document-term matrix to the model. We had above obtained the unique words of vocabulary using both TF-IDF and Count Vectorizer. We can continue with either as have the same unique words in both the obtained vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd3487e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33472667, 0.3342451 , 0.55513862, 0.55513862, 0.69666216,\n",
       "        0.55513862, 0.3342451 , 0.33430078, 0.33472667, 0.55513862,\n",
       "        0.33472667, 0.55513862, 0.3342451 , 0.55513862, 0.48132068,\n",
       "        0.55513862, 0.33430078, 0.55513862, 0.3342451 , 0.33430078,\n",
       "        0.55513862, 0.55513862, 0.33430078, 0.33430078, 0.55513862,\n",
       "        0.70816404, 0.55513862, 0.33472667, 0.3342451 , 0.33430078,\n",
       "        0.33430078, 0.33472667, 0.33430078, 0.55513862, 0.55513862,\n",
       "        0.3342451 , 0.3342451 , 0.3342451 , 0.55513862, 0.33430078,\n",
       "        0.55513862, 0.89793781, 0.7852696 , 0.33430078, 0.89793781,\n",
       "        0.3342451 , 0.3342451 , 0.55513862, 0.3342451 , 0.33430078,\n",
       "        0.3342451 , 0.3342451 ],\n",
       "       [0.67323173, 0.60845718, 0.33391127, 0.33391127, 0.55591404,\n",
       "        0.33391127, 0.60845718, 0.61339847, 0.67323173, 0.33391127,\n",
       "        0.67323173, 0.33391127, 0.60845718, 0.33391127, 0.75130981,\n",
       "        0.33391127, 0.61339847, 0.33391127, 0.60845718, 0.61339847,\n",
       "        0.33391127, 0.33391127, 0.61339847, 0.61339847, 0.33391127,\n",
       "        0.75422629, 0.33391127, 0.67323173, 0.60845718, 0.61339847,\n",
       "        0.61339847, 0.67323173, 0.61339847, 0.33391127, 0.33391127,\n",
       "        0.60845718, 0.60845718, 0.60845718, 0.33391127, 0.61339847,\n",
       "        0.33391127, 0.33481645, 0.88952985, 0.61339847, 0.33481645,\n",
       "        0.60845718, 0.60845718, 0.33391127, 0.60845718, 0.61339847,\n",
       "        0.60845718, 0.60845718],\n",
       "       [0.33502482, 0.33464782, 0.33426579, 0.33426579, 0.33552252,\n",
       "        0.33426579, 0.33464782, 0.3346026 , 0.33502482, 0.33426579,\n",
       "        0.33502482, 0.33426579, 0.33464782, 0.33426579, 0.33568732,\n",
       "        0.33426579, 0.3346026 , 0.33426579, 0.33464782, 0.3346026 ,\n",
       "        0.33426579, 0.33426579, 0.3346026 , 0.3346026 , 0.33426579,\n",
       "        0.33677601, 0.33426579, 0.33502482, 0.33464782, 0.3346026 ,\n",
       "        0.3346026 , 0.33502482, 0.3346026 , 0.33426579, 0.33426579,\n",
       "        0.33464782, 0.33464782, 0.33464782, 0.33426579, 0.3346026 ,\n",
       "        0.33426579, 0.33525982, 0.33690413, 0.3346026 , 0.33525982,\n",
       "        0.33464782, 0.33464782, 0.33426579, 0.33464782, 0.3346026 ,\n",
       "        0.33464782, 0.33464782]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create object for the LDA class \n",
    "# Inside this class LDA: define the components:\n",
    "lda_model = LatentDirichletAllocation(n_components = 3, max_iter = 20, random_state = 20)\n",
    "\n",
    "# fit transform on model on our count_vectorizer : running this will return our topics \n",
    "X_topics = lda_model.fit(tf_idf_arr)\n",
    "\n",
    "# .components_ gives us our topic distribution \n",
    "topic_words = lda_model.components_\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea806c5",
   "metadata": {},
   "source": [
    "### 4a. Retrieve the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7255ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['weekend' 'want' 'watch' 'movie']\n",
      "Topic 2 ['watch' 'movie' 'good' 'cricket']\n",
      "Topic 3 ['watch' 'movie' 'good' 'book']\n"
     ]
    }
   ],
   "source": [
    "#  Define the number of Words that we want to print in every topic : n_top_words\n",
    "n_top_words = 5\n",
    "\n",
    "for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "    # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "    sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "    # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "    topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "    # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "    # obtaining topics + words\n",
    "    # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "    topic_words = topic_words[:-n_top_words:-1]\n",
    "    print (\"Topic\", str(i+1), topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af52aa",
   "metadata": {},
   "source": [
    "### 4b. Annotating the topics the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d62088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1  -- Topic: 0\n",
      "Document 2  -- Topic: 1\n",
      "Document 3  -- Topic: 1\n",
      "Document 4  -- Topic: 1\n",
      "Document 5  -- Topic: 0\n"
     ]
    }
   ],
   "source": [
    "# To view what topics are assigned to the douments:\n",
    "\n",
    "doc_topic = lda_model.transform(tf_idf_arr)  \n",
    "\n",
    "# iterating over ever value till the end value\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    \n",
    "    # argmax() gives maximum index value\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    \n",
    "    # document is n+1  \n",
    "    print (\"Document\", n+1, \" -- Topic:\" ,topic_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
