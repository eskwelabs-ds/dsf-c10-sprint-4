{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d63184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in c:\\users\\rheyannmagcalas\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\rheyannmagcalas\\anaconda3\\lib\\site-packages (1.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n",
      "ERROR: Could not find a version that satisfies the requirement pychecker (from versions: none)\n",
      "ERROR: No matching distribution found for pychecker\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\rheyannmagcalas\\anaconda3\\lib\\site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\rheyannmagcalas\\anaconda3\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in c:\\users\\rheyannmagcalas\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\rheyannmagcalas\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordninja in c:\\users\\rheyannmagcalas\\anaconda3\\lib\\site-packages (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji\n",
    "!pip install emoji\n",
    "!pip install pychecker\n",
    "!pip install contractions\n",
    "!pip install wordninja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8748a",
   "metadata": {},
   "source": [
    "* Lowercasing the data\n",
    "* Fix Accented Words\n",
    "* Expanding Contractions\n",
    "* Removing emoticons\n",
    "* Removing mentions\n",
    "* Removing Hashtags\n",
    "* Removing Numbers and Special Characters\n",
    "* Removing extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ddbe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rheyannmagcalas\\AppData\\Local\\Temp\\ipykernel_4372\\540732830.py:10: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import demoji\n",
    "import emoji\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import wordninja\n",
    "\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90942dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a2f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('twitter_disaster_tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f38021d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this  # earthquake...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask .  Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to  ' shelter in place '  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive  # wildfires evacuation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby  # Alaska a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this  # earthquake...   \n",
       "1   4     NaN      NaN           Forest fire near La Ronge Sask .  Canada   \n",
       "2   5     NaN      NaN  All residents asked to  ' shelter in place '  ...   \n",
       "3   6     NaN      NaN  13,000 people receive  # wildfires evacuation ...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby  # Alaska a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccc4c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#show feature data types\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e169c",
   "metadata": {},
   "source": [
    "### References:\n",
    "* https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "* https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n",
    "* https://www.kaggle.com/takanorihasebe/text-cleaning-bert-and-transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94868219",
   "metadata": {},
   "source": [
    "### Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96974505",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b988141",
   "metadata": {},
   "source": [
    "### Expanding Contractions\n",
    "* Example: from “Y’all i’d contractions you’re expanded don’t think.” to you all I would contractions you are expanded do not think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0cba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    result = text\n",
    "    try:\n",
    "        result = contractions.text(text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    contractions = {\n",
    "        \"ain't\": \"am not / are not\",\n",
    "        \"aren't\": \"are not / am not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he had / he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he shall / he will\",\n",
    "        \"he'll've\": \"he shall have / he will have\",\n",
    "        \"he's\": \"he has / he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how has / how is\",\n",
    "        \"i'd\": \"I had / I would\",\n",
    "        \"i'd've\": \"I would have\",\n",
    "        \"i'll\": \"I shall / I will\",\n",
    "        \"i'll've\": \"I shall have / I will have\",\n",
    "        \"i'm\": \"I am\",\n",
    "        \"i've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it had / it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it shall / it will\",\n",
    "        \"it'll've\": \"it shall have / it will have\",\n",
    "        \"it's\": \"it has / it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she had / she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she shall / she will\",\n",
    "        \"she'll've\": \"she shall have / she will have\",\n",
    "        \"she's\": \"she has / she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so as / so is\",\n",
    "        \"that'd\": \"that would / that had\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that has / that is\",\n",
    "        \"there'd\": \"there had / there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there has / there is\",\n",
    "        \"they'd\": \"they had / they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they shall / they will\",\n",
    "        \"they'll've\": \"they shall have / they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we had / we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what shall / what will\",\n",
    "        \"what'll've\": \"what shall have / what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what has / what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when has / when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where has / where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who shall / who will\",\n",
    "        \"who'll've\": \"who shall have / who will have\",\n",
    "        \"who's\": \"who has / who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why has / why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you had / you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you shall / you will\",\n",
    "        \"you'll've\": \"you shall have / you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    for word in result.split():\n",
    "        if word.lower() in contractions:\n",
    "            result = result.replace(word, contractions[word.lower()])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec11bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda x: expand_contractions(str(x['text'])),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c63c7a",
   "metadata": {},
   "source": [
    "### Removing Accented Characters\n",
    "\n",
    "* Example: 'Sómě Áccěntěd těxt. Some words such as résumé, café, prótest, divorcé, coördinate, exposé, latté.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b59d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "data['text'] = data.apply(lambda x: remove_accented_chars(str(x['text'])),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a3ee6",
   "metadata": {},
   "source": [
    "### Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31db5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoji_count(post):\n",
    "    emoji = demoji.findall(post)\n",
    "    name = [k for k,v in emoji.items()]\n",
    "    return len(name)\n",
    "\n",
    "data['emoji_count'] = data.apply(lambda x: get_emoji_count(str(x['text'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5975277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return ''.join(c for c in str(text) if c not in emoji.EMOJI_DATA)\n",
    "\n",
    "data['text'] = data.apply(lambda x: remove_emoji(x['text']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870a1d1",
   "metadata": {},
   "source": [
    "### Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68249f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_hashtags(post):\n",
    "    sample_result = re.findall(\"#(\\s\\w+)\", post)\n",
    "    \n",
    "    return ','.join(sample_result), len(sample_result)\n",
    "\n",
    "def get_hashtags_count(post):\n",
    "    sample_result = re.findall(\"#(\\s\\w+)\", post)\n",
    "    \n",
    "    return len(sample_result)\n",
    "\n",
    "data['hashtags'] = data.apply(lambda x: get_hashtags(x['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a858b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rheyannmagcalas\\AppData\\Local\\Temp\\ipykernel_4372\\2332306136.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace('[＃#](\\s\\w+)', ' ', case=False)\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].str.replace('[＃#](\\s\\w+)', ' ', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78f558f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'our deeds are the reason of this    may allah forgive us all'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb863e78",
   "metadata": {},
   "source": [
    "### Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d01dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions(post):\n",
    "    sample_result = re.findall('@([a-zA-Z0-9]{1,15})', post)\n",
    "    \n",
    "    return sample_result\n",
    "\n",
    "def get_mentions_count(post):\n",
    "    sample_result = re.findall('@([a-zA-Z0-9]{1,15})', post)\n",
    "    \n",
    "    return len(sample_result)\n",
    "\n",
    "data['mentions'] = data.apply(lambda x: get_mentions(x['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b41efacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'doh']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mentions('please help us @who @doh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ff90e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rheyannmagcalas\\AppData\\Local\\Temp\\ipykernel_4372\\4263372094.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace('@([a-zA-Z0-9]{1,15})', ' ', case=False)\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].str.replace('@([a-zA-Z0-9]{1,15})', ' ', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b6236",
   "metadata": {},
   "source": [
    "### Remove hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eac69892",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    text = str(text)\n",
    "    if text != \"\":\n",
    "        return TAG_RE.sub(' ', text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1686a1a",
   "metadata": {},
   "source": [
    "### Remove Numbers and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "886055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    pattern = r'[^a-zA-Z]' \n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "data['text'] = data.apply(lambda x: remove_numbers(str(x['text'])),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a7e78",
   "metadata": {},
   "source": [
    "### Remove Extra Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8406c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_white_spaces(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "data['text'] = data.apply(lambda x: remove_extra_white_spaces(str(x['text'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e309df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('twitter_disaster_tweet_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
