{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8c9886",
   "metadata": {},
   "source": [
    "* Lowercasing the data\n",
    "* Remove Special Characters\n",
    "* Remove Whitespace\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "* Lemmatization/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99ac2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527e9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('twitter_disaster_tweet_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd1da74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this may allah for...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive evacuation orders in california</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby as smoke fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id keyword location  \\\n",
       "0           0   1     NaN      NaN   \n",
       "1           1   4     NaN      NaN   \n",
       "2           2   5     NaN      NaN   \n",
       "3           3   6     NaN      NaN   \n",
       "4           4   7     NaN      NaN   \n",
       "\n",
       "                                                text  target  emoji_count  \\\n",
       "0  our deeds are the reason of this may allah for...       1            0   \n",
       "1              forest fire near la ronge sask canada       1            0   \n",
       "2  all residents asked to shelter in place are be...       1            0   \n",
       "3    people receive evacuation orders in california        1            0   \n",
       "4  just got sent this photo from ruby as smoke fr...       1            0   \n",
       "\n",
       "  hashtags mentions  \n",
       "0  ('', 0)       []  \n",
       "1  ('', 0)       []  \n",
       "2  ('', 0)       []  \n",
       "3  ('', 0)       []  \n",
       "4  ('', 0)       []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e6790",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68b6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    split=re.split(\"\\W+\",text) \n",
    "    return split\n",
    "\n",
    "data['tokenized_text']=data['text'].apply(lambda x: tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63072632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this may allah for...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, may, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive evacuation orders in california</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[, people, receive, evacuation, orders, in, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby as smoke fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, as,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id keyword location  \\\n",
       "0           0   1     NaN      NaN   \n",
       "1           1   4     NaN      NaN   \n",
       "2           2   5     NaN      NaN   \n",
       "3           3   6     NaN      NaN   \n",
       "4           4   7     NaN      NaN   \n",
       "\n",
       "                                                text  target  emoji_count  \\\n",
       "0  our deeds are the reason of this may allah for...       1            0   \n",
       "1              forest fire near la ronge sask canada       1            0   \n",
       "2  all residents asked to shelter in place are be...       1            0   \n",
       "3    people receive evacuation orders in california        1            0   \n",
       "4  just got sent this photo from ruby as smoke fr...       1            0   \n",
       "\n",
       "  hashtags mentions                                     tokenized_text  \n",
       "0  ('', 0)       []  [our, deeds, are, the, reason, of, this, may, ...  \n",
       "1  ('', 0)       []      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  ('', 0)       []  [all, residents, asked, to, shelter, in, place...  \n",
       "3  ('', 0)       []  [, people, receive, evacuation, orders, in, ca...  \n",
       "4  ('', 0)       []  [just, got, sent, this, photo, from, ruby, as,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242464a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenized_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070f74e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'our deeds are the reason of this may allah forgive us all'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a0884",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e18c0f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac3f6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text=[word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data['clean']=data['tokenized_text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a4f85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this may allah for...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, may, ...</td>\n",
       "      <td>[deeds, reason, may, allah, forgive, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive evacuation orders in california</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[, people, receive, evacuation, orders, in, ca...</td>\n",
       "      <td>[, people, receive, evacuation, orders, califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby as smoke fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>('', 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, as,...</td>\n",
       "      <td>[got, sent, photo, ruby, smoke, pours, school, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id keyword location  \\\n",
       "0           0   1     NaN      NaN   \n",
       "1           1   4     NaN      NaN   \n",
       "2           2   5     NaN      NaN   \n",
       "3           3   6     NaN      NaN   \n",
       "4           4   7     NaN      NaN   \n",
       "\n",
       "                                                text  target  emoji_count  \\\n",
       "0  our deeds are the reason of this may allah for...       1            0   \n",
       "1              forest fire near la ronge sask canada       1            0   \n",
       "2  all residents asked to shelter in place are be...       1            0   \n",
       "3    people receive evacuation orders in california        1            0   \n",
       "4  just got sent this photo from ruby as smoke fr...       1            0   \n",
       "\n",
       "  hashtags mentions                                     tokenized_text  \\\n",
       "0  ('', 0)       []  [our, deeds, are, the, reason, of, this, may, ...   \n",
       "1  ('', 0)       []      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  ('', 0)       []  [all, residents, asked, to, shelter, in, place...   \n",
       "3  ('', 0)       []  [, people, receive, evacuation, orders, in, ca...   \n",
       "4  ('', 0)       []  [just, got, sent, this, photo, from, ruby, as,...   \n",
       "\n",
       "                                               clean  \n",
       "0           [deeds, reason, may, allah, forgive, us]  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [residents, asked, shelter, place, notified, o...  \n",
       "3  [, people, receive, evacuation, orders, califo...  \n",
       "4   [got, sent, photo, ruby, smoke, pours, school, ]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4fabc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenized_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae5d2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deeds', 'reason', 'may', 'allah', 'forgive', 'us']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ba7f6",
   "metadata": {},
   "source": [
    "### Lemmatization / Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d130b",
   "metadata": {},
   "source": [
    "Stemming and Lemmatizing is the process of reducing a word to its root form. The main purpose is to reduce variations of the same word, thereby reducing the corpus of words we include in the model. The difference between stemming and lemmatizing is that, stemming chops off the end of the word without taking into consideration the context of the word. Whereas, Lemmatizing considers the context of the word and shortens the word into its root form based on the dictionary definition. Stemming is a faster process compared to Lemmantizing. Hence, it a trade-off between speed and accuracy.\n",
    "\n",
    "Let’s consider the word “belief” for example. The different variations of believe can be believing, believed, believes, and believe ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "605c9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a89b08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believ\n",
      "believ\n",
      "believ\n",
      "believ\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('believe'))\n",
    "print(ps.stem('believing'))\n",
    "print(ps.stem('believed'))\n",
    "print(ps.stem('believes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4ff25d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believe\n",
      "believing\n",
      "believed\n",
      "belief\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('believe'))\n",
    "print(lemmatizer.lemmatize('believing'))\n",
    "print(lemmatizer.lemmatize('believed'))\n",
    "print(lemmatizer.lemmatize('believes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf71feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(word_list):\n",
    "    lemmatized_output = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "    return lemmatized_output\n",
    "\n",
    "data['clean_lemma']=data['clean'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c29fa1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('twitter_disaster_tweet_preprocess', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a78501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rheyannmagcalas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rheyannmagcalas\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['Oh', 'man', ',', 'this', 'is', 'pretty', 'cool', '.', 'We', 'will', 'do', 'more', 'such', 'things', '.']\n",
      "['Oh', 'man', ',', 'pretty', 'cool', '.', 'We', 'things', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "# random sentecnce with lot of stop words\n",
    "sample_text = \"Oh man, this is pretty cool. We will do more such things.\"\n",
    "text_tokens = word_tokenize(sample_text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "\n",
    "print(text_tokens)\n",
    "print(tokens_without_sw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
