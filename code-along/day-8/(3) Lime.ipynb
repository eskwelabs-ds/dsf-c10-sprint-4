{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407169d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef6ba2",
   "metadata": {},
   "source": [
    "* Reference: https://github.com/marcotcr/lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "# downloading the nltk data for preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# reading the data as pandas dataframe\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a153c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP pre-processing\n",
    "# remove urls, handles, and the hashtag from hashtags \n",
    "# (taken from https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression)\n",
    "def remove_urls(text):\n",
    "  new_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "  return new_text\n",
    "\n",
    "# make all text lowercase\n",
    "def text_lowercase(text): \n",
    "  return text.lower()\n",
    "\n",
    "# remove numbers\n",
    "def remove_numbers(text): \n",
    "  result = re.sub(r'\\d+', '', text) \n",
    "  return result\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text): \n",
    "  translator = str.maketrans('', '', string.punctuation)\n",
    "  return text.translate(translator)\n",
    "\n",
    "# function for all pre-processing steps\n",
    "def preprocessing(text):\n",
    "  text = text_lowercase(text)\n",
    "  text = remove_urls(text)\n",
    "  text = remove_numbers(text)\n",
    "  text = remove_punctuation(text)\n",
    "  return text\n",
    "\n",
    "# pre-processing the text body column\n",
    "pp_text = []\n",
    "for text_data in train['text']:\n",
    "  # check if string\n",
    "  if isinstance(text_data, str):\n",
    "    pp_text_data = preprocessing(text_data)\n",
    "    pp_text.append(pp_text_data)\n",
    "   # if not string\n",
    "  else:\n",
    "    pp_text.append(np.NaN)\n",
    "\n",
    "# add pre-processed column to dataset\n",
    "train['pp_text'] = pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeea3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[\"pp_text\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag-of-words with weights using tfid vectoriser\n",
    "# strip accents and remove stop words during vectorisation\n",
    "tf=(strip_accents = 'ascii', stop_words='english')\n",
    "\n",
    "# transform and fit the training set with vectoriser\n",
    "X_train_tf = tf.(X_train)\n",
    "# transform the test set with vectoriser\n",
    "X_test_tf = tf.(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic ------------------- model\n",
    "logreg = (verbose=1, random_state=0, penalty='l2', solver='newton-cg')\n",
    "# train model on  vectorised training data\n",
    "model = logreg.fit(X_train_tf, y_train)\n",
    "# evaluate model performance on the test set\n",
    "pred = model.predict(X_test_tf)\n",
    "sklearn.metrics.f1_score(y_test, pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ff670",
   "metadata": {},
   "source": [
    "### Using Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed41be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import lime\n",
    "import sklearn.ensemble\n",
    "from __future__ import print_function\n",
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import \n",
    "\n",
    "# converting the vectoriser and model into a pipeline\n",
    "# this is necessary as LIME takes a model pipeline as an input\n",
    "c = make_pipeline(tf, model)\n",
    "\n",
    "# saving a list of strings version of the X_test object\n",
    "ls_X_test= list(X_test)\n",
    "\n",
    "# saving the class names in a dictionary to increase interpretability\n",
    "class_names = {0: 'non-disaster', 1:'disaster'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_X_test= list(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_X_test[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the LIME explainer\n",
    "# add the class names for interpretability\n",
    "LIME_explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# choose a random single prediction\n",
    "idx = 15\n",
    "# explain the chosen prediction \n",
    "# use the probability results of the logistic regression\n",
    "# can also add num_features parameter to reduce the number of features explained\n",
    "LIME_exp = LIME_explainer.explain_instance(ls_X_test[idx], c.predict_proba)\n",
    "# print results\n",
    "print('Document id: %d' % idx)\n",
    "print('Tweet: ', ls_X_test[idx])\n",
    "print('Probability disaster =', c.predict_proba([ls_X_test[idx]]).round(3)[0,1])\n",
    "print('True class: %s' % class_names.get(list(y_test)[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1 = disaster class, 0 = non-disaster class\")\n",
    "# show the explainability results with highlighted text\n",
    "LIME_exp.show_in_notebook(text=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
