{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbe0313",
   "metadata": {},
   "source": [
    "Models that enable spaCy to predict linguistic attributes in context\n",
    "\n",
    "* Part-of-speech tags\n",
    "* Syntactic dependencies\n",
    "* Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906f045",
   "metadata": {},
   "source": [
    "Spacy Model: https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = (\"\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)\n",
    "    # print(token.text, token.lemma_, token.pos_, token.dep_,  token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba276c",
   "metadata": {},
   "source": [
    "#### POS Values\n",
    "\n",
    "* ADJ: adjective\n",
    "* ADP: adposition\n",
    "* ADV: adverb\n",
    "* AUX: auxiliary\n",
    "* CCONJ: coordinating conjunction\n",
    "* DET: determiner\n",
    "* INTJ: interjection\n",
    "* NOUN: noun\n",
    "* NUM: numeral\n",
    "* PART: particle\n",
    "* PRON: pronoun\n",
    "* PROPN: proper noun\n",
    "* PUNCT: punctuation\n",
    "* SCONJ: subordinating conjunction\n",
    "* SYM: symbol\n",
    "* VERB: verb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b82097",
   "metadata": {},
   "source": [
    "#### Use Case of POS:\n",
    "\n",
    "* Text Cleaning\n",
    "* Feature Engineering tasks\n",
    "* Word sense disambiguation\n",
    "\n",
    "https://spacy.io/api/doc#init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "?doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a41486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Eskwelabs is an edtech startup based in Manila that provides online courses on data science and analytics to help people in underserved communities in the Philippines, such as stay-at-home moms and young people without university degrees, find better work in the digital age. Since its launch in 2019, Eskwelabs taught more than 3,000 people, about 90% of whom found better-paying work within 90 days of completing its course.\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc.:\n",
    "    print('---------------------------')\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import \n",
    "\n",
    "(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c378c18",
   "metadata": {},
   "source": [
    "### Text Generation: Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ee6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import markovify\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import warnings\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae1c70",
   "metadata": {},
   "source": [
    "#### Inspect Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597408a",
   "metadata": {},
   "source": [
    "#### Import novels as text objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d28927",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = gutenberg.('shakespeare-hamlet.txt')\n",
    "macbeth = gutenberg.('shakespeare-macbeth.txt')\n",
    "caesar = gutenberg.('shakespeare-caesar.txt')\n",
    "print('\\nRaw:\\n', hamlet[:100])\n",
    "print('\\nRaw:\\n', macbeth[:100])\n",
    "print('\\nRaw:\\n', caesar[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0bd3b4",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7044d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub('[\\[].*?[\\]]', '', text)\n",
    "    text = re.sub(r'(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b','', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb51a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = re.sub(r'Chapter \\d+', '', hamlet)\n",
    "macbeth = re.sub(r'Chapter \\d+', '', macbeth)\n",
    "caesar = re.sub(r'Chapter \\d+', '', caesar)\n",
    "\n",
    "hamlet = text_cleaner(hamlet)\n",
    "caesar = text_cleaner(caesar)\n",
    "macbeth = text_cleaner(macbeth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d62c13",
   "metadata": {},
   "source": [
    "#### Initialize Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81607ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = \n",
    "hamlet_doc = (hamlet)\n",
    "macbeth_doc = (macbeth)\n",
    "caesar_doc = (caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413dfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_sents = ' '.join([sent.text for sent in hamlet_doc.sents if len(sent.text) > 1])\n",
    "macbeth_sents = ' '.join([sent.text for sent in macbeth_doc.sents if len(sent.text) > 1])\n",
    "caesar_sents = ' '.join([sent.text for sent in caesar_doc.sents if len(sent.text) > 1])\n",
    "\n",
    "shakespeare_sents = hamlet_sents + macbeth_sents + caesar_sents\n",
    "print(shakespeare_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2ce26",
   "metadata": {},
   "source": [
    "#### Create text generator using markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde52547",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_1 = (shakespeare_sents, state_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7133c",
   "metadata": {},
   "source": [
    "#### We will randomly generate five sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc568ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Longer sentences:\")\n",
    "for i in range(5):\n",
    "    print(generator_1.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae664c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shorter sentences:\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(generator_1.make_short_sentence(max_chars=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dff721",
   "metadata": {},
   "source": [
    "#### Improving the result using POSifiedText Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSifiedText(markovify.Text):   \n",
    "    def word_split(self, sentence):\n",
    "        return ['::'.join((word.text, word.pos_)) for word in nlp(sentence)]   \n",
    "    \n",
    "    def word_join(self, words):\n",
    "        sentence = ' '.join(word.split('::')[0] for word in words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "generator_2 = POSifiedText(shakespeare_sents, state_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc58e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Longer sentences:\")\n",
    "\n",
    "#now we will use the above generator to generate sentences\n",
    "for i in range(5):\n",
    "    print(generator_2.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14042a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shorter sentences:\")\n",
    "#print 100 characters or less sentences\n",
    "for i in range(5):\n",
    "    print(generator_2.make_short_sentence(max_chars=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b966f69",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1f5b3",
   "metadata": {},
   "source": [
    "#### Number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the \"en_core_web_sm\" pipeline\n",
    "nlp = ____\n",
    "\n",
    "text = \"Itâ€™s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = ____\n",
    "\n",
    "# Print the document text\n",
    "print(____.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a26ae",
   "metadata": {},
   "source": [
    "#### Number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20682935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Itâ€™s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = ____\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = ____.____\n",
    "    token_pos = ____.____\n",
    "    token_dep = ____.____\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
